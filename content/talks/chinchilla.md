+++
title = "Compute-optimal large language models"
presenter = "Michael Scherbela"
talk_date = 2023-01-18T10:30:00+00:00
slides = "2023-01-18_Chinchilla.pdf"
+++

When training a LLM with a fixed compute-budget, a key tradeoff is how many parameters to use vs. how many tokens to process during training. This paper by DeepMind shows that historically LLMs were scaled up to quickly in parameters and insufficiently in the amount of training data.

https://arxiv.org/abs/2203.15556.pdf

