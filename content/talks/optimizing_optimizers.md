+++
title = "Optimizing Optimizers"
presenter = "Michael Scherbela"
talk_date = 2023-03-22T11:00:00+00:00
slides = "2023-03-22_Optimizing_Optimizers.pdf"
+++

A short overview of 3 papers, which each try to find better first order optimizers, by optimizing their optimizers:

- [Gradient Descent: The Ultimate Optimizer, NeurIPS 2022](https://openreview.net/forum?id=-Qp-3L-5ZdI): Optimization of SGD-hyper-parameters using SGD
- [Symbolic Discover of Optimization Algorithms, 2023](http://arxiv.org/abs/2302.06675): Evolutionary search for new optimizers, discovers a simple, robust optimizer which obtains new SOTA on ImageNet
- [Learning to learn by gradient descent by gradient descent, NIPS 2016](https://proceedings.neurips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html): Replacing the optimizer by a neural network
